# 05-Performance King - Database Optimization Master Class

Bhai, ab **SQL performance ka asli game** sikhenge! ðŸ”¥ **High-traffic applications** jaise **Amazon, Flipkart** ka database optimize karenge aur **millions of users** ko handle karne wale **production-level techniques** master karenge!

Real scenario: Tumhe **Performance Engineer** banana hai jo **database bottlenecks** solve kare! **Peak traffic** handle kare! **Query speed** 100x improve kare! **Industry secrets** ke saath! âš¡

---

## ðŸŽ¯ **What We'll Optimize: E-commerce Platform**

### **Performance Challenges:**
- **Million+ concurrent users** during sales
- **Complex product searches** with filters
- **Real-time inventory updates**
- **Payment processing** under load
- **Analytics dashboards** without lag

### **Performance Metrics:**
- Query execution time < 100ms
- 99.9% uptime during peak traffic
- Database response time < 50ms
- Memory utilization < 80%
- CPU usage optimization

---

## ðŸ—ï¸ **High-Performance E-commerce Schema**

### **PRODUCTS Table (Optimized):**
```sql
CREATE TABLE products (
    product_id INT PRIMARY KEY AUTO_INCREMENT,
    sku VARCHAR(20) UNIQUE NOT NULL,
    name VARCHAR(200) NOT NULL,
    brand_id INT NOT NULL,
    category_id INT NOT NULL,
    price DECIMAL(10,2) NOT NULL,
    discount_price DECIMAL(10,2),
    stock_quantity INT NOT NULL DEFAULT 0,
    is_active BOOLEAN DEFAULT TRUE,
    rating_avg DECIMAL(3,2) DEFAULT 0.00,
    review_count INT DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    
    -- Performance Indexes
    INDEX idx_active_category (is_active, category_id),
    INDEX idx_price_range (price, discount_price),
    INDEX idx_brand_active (brand_id, is_active),
    INDEX idx_rating_reviews (rating_avg, review_count),
    INDEX idx_stock_active (stock_quantity, is_active),
    INDEX idx_created_desc (created_at DESC),
    
    -- Composite indexes for common queries
    INDEX idx_category_price_rating (category_id, price, rating_avg),
    INDEX idx_brand_category_active (brand_id, category_id, is_active),
    INDEX idx_search_optimization (name, brand_id, category_id, is_active),
    
    FOREIGN KEY (brand_id) REFERENCES brands(brand_id),
    FOREIGN KEY (category_id) REFERENCES categories(category_id)
) ENGINE=InnoDB 
  ROW_FORMAT=COMPRESSED 
  KEY_BLOCK_SIZE=8;
```

### **ORDERS Table (High-Performance):**
```sql
CREATE TABLE orders (
    order_id INT PRIMARY KEY AUTO_INCREMENT,
    order_number VARCHAR(20) UNIQUE NOT NULL,
    customer_id INT NOT NULL,
    order_status ENUM('pending', 'confirmed', 'shipped', 'delivered', 'cancelled') DEFAULT 'pending',
    payment_status ENUM('pending', 'paid', 'failed', 'refunded') DEFAULT 'pending',
    order_total DECIMAL(12,2) NOT NULL,
    order_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    delivery_date TIMESTAMP NULL,
    
    -- Partitioning by date for better performance
    -- Performance indexes
    INDEX idx_customer_date (customer_id, order_date),
    INDEX idx_status_date (order_status, order_date),
    INDEX idx_payment_status (payment_status, order_date),
    INDEX idx_date_total (order_date, order_total),
    
    FOREIGN KEY (customer_id) REFERENCES customers(customer_id)
) ENGINE=InnoDB
  PARTITION BY RANGE (YEAR(order_date)) (
    PARTITION p_2023 VALUES LESS THAN (2024),
    PARTITION p_2024 VALUES LESS THAN (2025),
    PARTITION p_2025 VALUES LESS THAN (2026),
    PARTITION p_future VALUES LESS THAN MAXVALUE
  );
```

### **TRANSACTIONS Table (Optimized for Speed):**
```sql
CREATE TABLE transactions (
    transaction_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    order_id INT NOT NULL,
    payment_method ENUM('card', 'upi', 'netbanking', 'wallet', 'cod') NOT NULL,
    amount DECIMAL(12,2) NOT NULL,
    currency_code CHAR(3) DEFAULT 'INR',
    transaction_status ENUM('initiated', 'processing', 'success', 'failed', 'timeout') DEFAULT 'initiated',
    gateway_response TEXT,
    transaction_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    processed_at TIMESTAMP NULL,
    
    -- Memory engine for frequently accessed recent transactions
    INDEX idx_order_status (order_id, transaction_status),
    INDEX idx_date_status (transaction_date, transaction_status),
    INDEX idx_method_date (payment_method, transaction_date),
    INDEX idx_amount_date (amount, transaction_date),
    
    FOREIGN KEY (order_id) REFERENCES orders(order_id)
) ENGINE=InnoDB;

-- Separate table for recent transactions (in-memory for speed)
CREATE TABLE recent_transactions (
    transaction_id BIGINT PRIMARY KEY,
    order_id INT NOT NULL,
    amount DECIMAL(12,2) NOT NULL,
    transaction_status ENUM('initiated', 'processing', 'success', 'failed', 'timeout'),
    transaction_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_recent_status (transaction_status, transaction_date),
    INDEX idx_recent_order (order_id)
) ENGINE=MEMORY;
```

---

## âš¡ **INDEXING STRATEGIES - Speed Optimization**

### **1. Index Types & When to Use:**

#### **B-Tree Indexes (Default - Most Common):**
```sql
-- Single column index
CREATE INDEX idx_product_name ON products(name);

-- Composite index (order matters!)
CREATE INDEX idx_category_brand_price ON products(category_id, brand_id, price);

-- Covering index (includes all needed columns)
CREATE INDEX idx_product_search_covering 
ON products(category_id, brand_id, price, name, rating_avg, stock_quantity);
```

#### **Hash Indexes (For Exact Matches):**
```sql
-- Perfect for PRIMARY KEY lookups and ENUM fields
CREATE INDEX idx_payment_method_hash ON transactions(payment_method) USING HASH;
CREATE INDEX idx_order_status_hash ON orders(order_status) USING HASH;
```

#### **Full-Text Indexes (For Search):**
```sql
-- Product search optimization
ALTER TABLE products ADD FULLTEXT(name, description);

-- Advanced full-text search
CREATE INDEX idx_product_search_fulltext ON products(name, description, tags) 
WITH PARSER ngram;
```

### **2. Real E-commerce Query Optimizations:**

#### **Product Search Performance:**
```sql
-- Before optimization (SLOW)
EXPLAIN SELECT * FROM products 
WHERE name LIKE '%iphone%' 
  AND category_id = 1 
  AND price BETWEEN 50000 AND 100000
  AND is_active = TRUE;

-- After optimization (FAST)
-- Create optimized index
CREATE INDEX idx_search_optimized ON products(
    category_id, 
    is_active, 
    price, 
    name(10)  -- Prefix index for name
);

-- Optimized query
EXPLAIN SELECT product_id, name, price, rating_avg, stock_quantity
FROM products 
WHERE category_id = 1 
  AND is_active = TRUE
  AND price BETWEEN 50000 AND 100000
  AND MATCH(name) AGAINST('iphone' IN NATURAL LANGUAGE MODE)
ORDER BY rating_avg DESC
LIMIT 20;
```

#### **Order History Performance:**
```sql
-- Before: Slow customer order history
SELECT o.order_number, o.order_total, o.order_date, o.order_status
FROM orders o
WHERE o.customer_id = 12345
ORDER BY o.order_date DESC;

-- After: Optimized with proper indexing
CREATE INDEX idx_customer_orders_optimized ON orders(
    customer_id, 
    order_date DESC, 
    order_status
);

-- Query now uses index scan instead of filesort
EXPLAIN SELECT order_number, order_total, order_date, order_status
FROM orders 
WHERE customer_id = 12345
ORDER BY order_date DESC
LIMIT 10;
```

### **3. Index Monitoring & Maintenance:**

#### **Find Unused Indexes:**
```sql
-- Check index usage statistics
SELECT 
    OBJECT_SCHEMA,
    OBJECT_NAME,
    INDEX_NAME,
    COUNT_FETCH,
    COUNT_INSERT,
    COUNT_UPDATE,
    COUNT_DELETE
FROM performance_schema.table_io_waits_summary_by_index_usage 
WHERE OBJECT_SCHEMA = 'ecommerce'
  AND COUNT_FETCH = 0
ORDER BY OBJECT_NAME, INDEX_NAME;
```

#### **Index Size Analysis:**
```sql
-- Monitor index sizes
SELECT 
    TABLE_NAME,
    INDEX_NAME,
    ROUND(((INDEX_LENGTH) / 1024 / 1024), 2) AS 'INDEX_SIZE_MB',
    CARDINALITY,
    ROUND(CARDINALITY / (DATA_LENGTH / 1024), 2) AS 'SELECTIVITY'
FROM information_schema.STATISTICS s
JOIN information_schema.TABLES t ON s.TABLE_NAME = t.TABLE_NAME
WHERE s.TABLE_SCHEMA = 'ecommerce'
  AND t.TABLE_SCHEMA = 'ecommerce'
ORDER BY INDEX_SIZE_MB DESC;
```

---

## ðŸš€ **QUERY OPTIMIZATION TECHNIQUES**

### **1. EXPLAIN Analysis - Query Detective Work:**

#### **Reading EXPLAIN Output:**
```sql
-- Analyze slow query
EXPLAIN FORMAT=JSON
SELECT 
    p.name,
    p.price,
    p.rating_avg,
    b.brand_name,
    c.category_name,
    COUNT(r.review_id) as review_count
FROM products p
JOIN brands b ON p.brand_id = b.brand_id
JOIN categories c ON p.category_id = c.category_id
LEFT JOIN reviews r ON p.product_id = r.product_id
WHERE p.price BETWEEN 10000 AND 50000
  AND p.is_active = TRUE
  AND p.rating_avg >= 4.0
GROUP BY p.product_id
ORDER BY p.rating_avg DESC, review_count DESC
LIMIT 20;
```

#### **Query Optimization Steps:**
```sql
-- Step 1: Add covering index
CREATE INDEX idx_product_price_rating_covering ON products(
    price, 
    is_active, 
    rating_avg, 
    product_id, 
    name, 
    brand_id, 
    category_id
);

-- Step 2: Optimize JOINs with proper indexes
CREATE INDEX idx_brand_lookup ON brands(brand_id, brand_name);
CREATE INDEX idx_category_lookup ON categories(category_id, category_name);
CREATE INDEX idx_reviews_product ON reviews(product_id);

-- Step 3: Rewrite query for better performance
SELECT 
    p.name,
    p.price,
    p.rating_avg,
    b.brand_name,
    c.category_name,
    p.review_count  -- Pre-calculated field
FROM products p
STRAIGHT_JOIN brands b ON p.brand_id = b.brand_id
STRAIGHT_JOIN categories c ON p.category_id = c.category_id
WHERE p.price BETWEEN 10000 AND 50000
  AND p.is_active = TRUE
  AND p.rating_avg >= 4.0
ORDER BY p.rating_avg DESC, p.review_count DESC
LIMIT 20;
```

### **2. Join Optimization:**

#### **Nested Loop vs Hash Join:**
```sql
-- Force optimal join type
SELECT /*+ USE_INDEX(p idx_product_price_rating) */
    p.product_id,
    p.name,
    o.order_count,
    o.total_revenue
FROM products p
INNER JOIN (
    SELECT 
        oi.product_id,
        COUNT(*) as order_count,
        SUM(oi.price * oi.quantity) as total_revenue
    FROM order_items oi
    INNER JOIN orders o ON oi.order_id = o.order_id
    WHERE o.order_date >= DATE_SUB(CURRENT_DATE, INTERVAL 30 DAY)
      AND o.order_status = 'delivered'
    GROUP BY oi.product_id
    HAVING order_count >= 10
) o ON p.product_id = o.product_id
WHERE p.is_active = TRUE
ORDER BY o.total_revenue DESC
LIMIT 50;
```

### **3. Subquery vs JOIN Performance:**

#### **Optimizing EXISTS vs JOIN:**
```sql
-- Slow: EXISTS subquery
SELECT p.product_id, p.name, p.price
FROM products p
WHERE EXISTS (
    SELECT 1 
    FROM order_items oi 
    JOIN orders o ON oi.order_id = o.order_id
    WHERE oi.product_id = p.product_id 
      AND o.order_date >= DATE_SUB(CURRENT_DATE, INTERVAL 7 DAY)
);

-- Fast: JOIN with GROUP BY
SELECT DISTINCT p.product_id, p.name, p.price
FROM products p
INNER JOIN order_items oi ON p.product_id = oi.product_id
INNER JOIN orders o ON oi.order_id = o.order_id
WHERE o.order_date >= DATE_SUB(CURRENT_DATE, INTERVAL 7 DAY)
  AND p.is_active = TRUE;

-- Even faster: Pre-aggregated table
CREATE TABLE recent_popular_products AS
SELECT DISTINCT oi.product_id
FROM order_items oi
INNER JOIN orders o ON oi.order_id = o.order_id
WHERE o.order_date >= DATE_SUB(CURRENT_DATE, INTERVAL 7 DAY);

-- Query becomes super fast
SELECT p.product_id, p.name, p.price
FROM products p
INNER JOIN recent_popular_products rpp ON p.product_id = rpp.product_id
WHERE p.is_active = TRUE;
```

---

## ðŸ“Š **DATABASE CONFIGURATION TUNING**

### **1. MySQL Configuration for High Performance:**

#### **Key Configuration Parameters:**
```ini
# /etc/mysql/mysql.conf.d/mysqld.cnf

[mysqld]
# Memory Settings
innodb_buffer_pool_size = 4G          # 70-80% of available RAM
innodb_log_file_size = 512M            # 25% of buffer pool size
innodb_log_buffer_size = 64M           # For high write loads
query_cache_size = 256M                # For read-heavy workloads
query_cache_type = 1

# Connection Settings
max_connections = 1000                 # Adjust based on load
max_connect_errors = 10000
connect_timeout = 10
interactive_timeout = 28800
wait_timeout = 28800

# InnoDB Performance
innodb_flush_log_at_trx_commit = 2     # Better performance, slight risk
innodb_flush_method = O_DIRECT        # Avoid double buffering
innodb_file_per_table = 1             # Better space management
innodb_io_capacity = 2000             # SSD optimization
innodb_read_io_threads = 8
innodb_write_io_threads = 8

# Query Cache and Temporary Tables
tmp_table_size = 256M
max_heap_table_size = 256M
sort_buffer_size = 4M
read_buffer_size = 2M
read_rnd_buffer_size = 8M

# Logging for Performance Analysis
slow_query_log = 1
slow_query_log_file = /var/log/mysql/slow.log
long_query_time = 0.5                 # Log queries > 0.5 seconds
log_queries_not_using_indexes = 1
```

### **2. Connection Pool Optimization:**

#### **Application-Level Connection Pooling:**
```python
# Python example with connection pooling
import mysql.connector.pooling

# Configure connection pool
config = {
    'user': 'ecommerce_user',
    'password': 'secure_password',
    'host': 'localhost',
    'database': 'ecommerce',
    'pool_name': 'ecommerce_pool',
    'pool_size': 20,              # Adjust based on load
    'pool_reset_session': True,
    'autocommit': True,
    'charset': 'utf8mb4',
    'use_unicode': True,
    'connect_timeout': 10,
    'sql_mode': 'STRICT_TRANS_TABLES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION'
}

# Create connection pool
pool = mysql.connector.pooling.MySQLConnectionPool(**config)

# Efficient query execution
def get_products_optimized(category_id, price_min, price_max):
    connection = pool.get_connection()
    try:
        cursor = connection.cursor(prepared=True)  # Use prepared statements
        query = """
        SELECT product_id, name, price, rating_avg, stock_quantity
        FROM products 
        WHERE category_id = ? 
          AND price BETWEEN ? AND ? 
          AND is_active = TRUE
          AND stock_quantity > 0
        ORDER BY rating_avg DESC, review_count DESC
        LIMIT 20
        """
        cursor.execute(query, (category_id, price_min, price_max))
        return cursor.fetchall()
    finally:
        cursor.close()
        connection.close()  # Returns to pool
```

---

## ðŸ”¥ **REAL-WORLD PERFORMANCE SCENARIOS**

### **1. Flash Sale Traffic Handling:**

#### **Pre-Sale Optimization:**
```sql
-- Create dedicated tables for flash sale
CREATE TABLE flash_sale_products (
    product_id INT PRIMARY KEY,
    original_price DECIMAL(10,2),
    sale_price DECIMAL(10,2),
    available_quantity INT,
    max_per_customer INT DEFAULT 1,
    sale_start_time TIMESTAMP,
    sale_end_time TIMESTAMP,
    
    INDEX idx_sale_active (sale_start_time, sale_end_time),
    INDEX idx_availability (available_quantity)
) ENGINE=MEMORY;  -- In-memory for ultra-fast access

-- Pre-load sale data
INSERT INTO flash_sale_products 
SELECT 
    p.product_id,
    p.price,
    p.price * 0.5,  -- 50% off
    p.stock_quantity,
    1,
    '2024-07-15 12:00:00',
    '2024-07-15 14:00:00'
FROM products p
WHERE p.product_id IN (1001, 1002, 1003, 1004, 1005);

-- Ultra-fast sale query
SELECT 
    fsp.product_id,
    p.name,
    fsp.original_price,
    fsp.sale_price,
    fsp.available_quantity
FROM flash_sale_products fsp
INNER JOIN products p ON fsp.product_id = p.product_id
WHERE CURRENT_TIMESTAMP BETWEEN fsp.sale_start_time AND fsp.sale_end_time
  AND fsp.available_quantity > 0
ORDER BY fsp.sale_price ASC;
```

#### **Inventory Update Optimization:**
```sql
-- Atomic inventory updates to prevent overselling
DELIMITER //
CREATE PROCEDURE UpdateInventoryAtomic(
    IN p_product_id INT,
    IN p_quantity INT,
    OUT p_success BOOLEAN
)
BEGIN
    DECLARE v_current_stock INT DEFAULT 0;
    
    START TRANSACTION;
    
    -- Lock row for update
    SELECT available_quantity INTO v_current_stock
    FROM flash_sale_products 
    WHERE product_id = p_product_id
    FOR UPDATE;
    
    IF v_current_stock >= p_quantity THEN
        UPDATE flash_sale_products 
        SET available_quantity = available_quantity - p_quantity
        WHERE product_id = p_product_id;
        
        SET p_success = TRUE;
        COMMIT;
    ELSE
        SET p_success = FALSE;
        ROLLBACK;
    END IF;
END //
DELIMITER ;
```

### **2. Analytics Dashboard Performance:**

#### **Real-time Dashboard Optimization:**
```sql
-- Create materialized views for dashboard metrics
CREATE TABLE dashboard_metrics (
    metric_date DATE PRIMARY KEY,
    total_orders INT DEFAULT 0,
    total_revenue DECIMAL(15,2) DEFAULT 0,
    total_customers INT DEFAULT 0,
    avg_order_value DECIMAL(10,2) DEFAULT 0,
    top_selling_category VARCHAR(100),
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    
    INDEX idx_metric_date_desc (metric_date DESC)
);

-- Scheduled job to update metrics (runs every 5 minutes)
DELIMITER //
CREATE EVENT UpdateDashboardMetrics
ON SCHEDULE EVERY 5 MINUTE
DO
BEGIN
    INSERT INTO dashboard_metrics (metric_date, total_orders, total_revenue, total_customers, avg_order_value)
    SELECT 
        DATE(order_date),
        COUNT(*),
        SUM(order_total),
        COUNT(DISTINCT customer_id),
        AVG(order_total)
    FROM orders 
    WHERE order_date >= CURDATE()
    GROUP BY DATE(order_date)
    ON DUPLICATE KEY UPDATE
        total_orders = VALUES(total_orders),
        total_revenue = VALUES(total_revenue),
        total_customers = VALUES(total_customers),
        avg_order_value = VALUES(avg_order_value),
        updated_at = CURRENT_TIMESTAMP;
END //
DELIMITER ;

-- Dashboard query (now super fast)
SELECT 
    metric_date,
    total_orders,
    total_revenue,
    total_customers,
    avg_order_value,
    (total_revenue - LAG(total_revenue) OVER (ORDER BY metric_date)) / LAG(total_revenue) OVER (ORDER BY metric_date) * 100 AS revenue_growth_percent
FROM dashboard_metrics 
WHERE metric_date >= DATE_SUB(CURDATE(), INTERVAL 30 DAY)
ORDER BY metric_date DESC;
```

### **3. Search Performance Optimization:**

#### **Elasticsearch Integration for Product Search:**
```sql
-- Create optimized search index table
CREATE TABLE product_search_index (
    product_id INT PRIMARY KEY,
    search_text TEXT,
    category_path VARCHAR(500),
    brand_name VARCHAR(100),
    price DECIMAL(10,2),
    rating_avg DECIMAL(3,2),
    review_count INT,
    stock_quantity INT,
    is_active BOOLEAN,
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    
    FULLTEXT KEY ft_search_text (search_text),
    INDEX idx_category_price (category_path(100), price),
    INDEX idx_brand_rating (brand_name, rating_avg),
    INDEX idx_active_stock (is_active, stock_quantity)
) ENGINE=InnoDB;

-- Populate search index
INSERT INTO product_search_index
SELECT 
    p.product_id,
    CONCAT(p.name, ' ', p.description, ' ', b.brand_name, ' ', c.category_name) as search_text,
    CONCAT(c.parent_category, ' > ', c.category_name) as category_path,
    b.brand_name,
    p.price,
    p.rating_avg,
    p.review_count,
    p.stock_quantity,
    p.is_active,
    NOW()
FROM products p
JOIN brands b ON p.brand_id = b.brand_id
JOIN categories c ON p.category_id = c.category_id;

-- High-performance search query
SELECT 
    psi.product_id,
    SUBSTRING(psi.search_text, 1, 100) as product_name,
    psi.price,
    psi.rating_avg,
    psi.review_count,
    MATCH(psi.search_text) AGAINST('iphone 15 pro' IN NATURAL LANGUAGE MODE) AS relevance_score
FROM product_search_index psi
WHERE MATCH(psi.search_text) AGAINST('iphone 15 pro' IN NATURAL LANGUAGE MODE)
  AND psi.is_active = TRUE
  AND psi.stock_quantity > 0
ORDER BY relevance_score DESC, psi.rating_avg DESC
LIMIT 20;
```

---

## ðŸ“ˆ **MONITORING & PERFORMANCE ANALYSIS**

### **1. Real-time Performance Monitoring:**

#### **Key Performance Metrics to Track:**
```sql
-- Query performance monitoring
SELECT 
    DIGEST_TEXT as query_pattern,
    COUNT_STAR as execution_count,
    ROUND(AVG_TIMER_WAIT/1000000000, 3) as avg_execution_time_sec,
    ROUND(MAX_TIMER_WAIT/1000000000, 3) as max_execution_time_sec,
    ROUND(SUM_ROWS_EXAMINED/COUNT_STAR, 0) as avg_rows_examined,
    ROUND(SUM_ROWS_SENT/COUNT_STAR, 0) as avg_rows_returned
FROM performance_schema.events_statements_summary_by_digest 
WHERE SCHEMA_NAME = 'ecommerce'
ORDER BY avg_execution_time_sec DESC
LIMIT 20;

-- Connection monitoring
SELECT 
    processlist_id,
    processlist_user,
    processlist_host,
    processlist_db,
    processlist_command,
    processlist_time,
    processlist_state,
    LEFT(processlist_info, 100) as query_preview
FROM performance_schema.processlist 
WHERE processlist_command != 'Sleep'
ORDER BY processlist_time DESC;

-- Buffer pool hit ratio (should be > 95%)
SELECT 
    ROUND(
        (SELECT VARIABLE_VALUE FROM performance_schema.global_status WHERE VARIABLE_NAME = 'Innodb_buffer_pool_read_requests') /
        (SELECT VARIABLE_VALUE FROM performance_schema.global_status WHERE VARIABLE_NAME = 'Innodb_buffer_pool_read_requests' + 
         SELECT VARIABLE_VALUE FROM performance_schema.global_status WHERE VARIABLE_NAME = 'Innodb_buffer_pool_reads') * 100, 2
    ) as buffer_pool_hit_ratio_percent;
```

### **2. Automated Performance Alerts:**

#### **Performance Alert System:**
```sql
-- Create performance alerts table
CREATE TABLE performance_alerts (
    alert_id INT PRIMARY KEY AUTO_INCREMENT,
    alert_type ENUM('slow_query', 'high_cpu', 'low_hit_ratio', 'connection_limit') NOT NULL,
    alert_message TEXT NOT NULL,
    metric_value DECIMAL(10,2),
    threshold_value DECIMAL(10,2),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    resolved_at TIMESTAMP NULL,
    
    INDEX idx_alert_type_date (alert_type, created_at)
);

-- Stored procedure for performance monitoring
DELIMITER //
CREATE PROCEDURE CheckPerformanceMetrics()
BEGIN
    DECLARE v_slow_queries INT DEFAULT 0;
    DECLARE v_buffer_hit_ratio DECIMAL(5,2) DEFAULT 0;
    DECLARE v_active_connections INT DEFAULT 0;
    
    -- Check slow queries
    SELECT COUNT(*) INTO v_slow_queries
    FROM performance_schema.events_statements_summary_by_digest
    WHERE AVG_TIMER_WAIT/1000000000 > 1.0;  -- Queries taking more than 1 second
    
    IF v_slow_queries > 10 THEN
        INSERT INTO performance_alerts (alert_type, alert_message, metric_value, threshold_value)
        VALUES ('slow_query', CONCAT('Found ', v_slow_queries, ' slow queries'), v_slow_queries, 10);
    END IF;
    
    -- Check buffer pool hit ratio
    SELECT ROUND(
        (SELECT VARIABLE_VALUE FROM performance_schema.global_status WHERE VARIABLE_NAME = 'Innodb_buffer_pool_read_requests') /
        (SELECT VARIABLE_VALUE FROM performance_schema.global_status WHERE VARIABLE_NAME = 'Innodb_buffer_pool_read_requests' + 
         SELECT VARIABLE_VALUE FROM performance_schema.global_status WHERE VARIABLE_NAME = 'Innodb_buffer_pool_reads') * 100, 2
    ) INTO v_buffer_hit_ratio;
    
    IF v_buffer_hit_ratio < 95.0 THEN
        INSERT INTO performance_alerts (alert_type, alert_message, metric_value, threshold_value)
        VALUES ('low_hit_ratio', 'Buffer pool hit ratio is low', v_buffer_hit_ratio, 95.0);
    END IF;
    
    -- Check connection count
    SELECT COUNT(*) INTO v_active_connections
    FROM performance_schema.processlist 
    WHERE processlist_command != 'Sleep';
    
    IF v_active_connections > 800 THEN  -- Alert when nearing max_connections
        INSERT INTO performance_alerts (alert_type, alert_message, metric_value, threshold_value)
        VALUES ('connection_limit', 'High number of active connections', v_active_connections, 800);
    END IF;
END //
DELIMITER ;

-- Schedule performance monitoring every minute
CREATE EVENT PerformanceMonitoring
ON SCHEDULE EVERY 1 MINUTE
DO CALL CheckPerformanceMetrics();
```

---

## ðŸŽ¯ **CACHING STRATEGIES**

### **1. Query Result Caching:**

#### **Redis Integration for Caching:**
```python
import redis
import json
import mysql.connector
from datetime import timedelta

# Redis connection
redis_client = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)

def get_popular_products_cached(category_id, cache_ttl=300):  # 5 minutes cache
    cache_key = f"popular_products_{category_id}"
    
    # Try to get from cache first
    cached_result = redis_client.get(cache_key)
    if cached_result:
        return json.loads(cached_result)
    
    # If not in cache, query database
    connection = mysql.connector.connect(**db_config)
    cursor = connection.cursor(dictionary=True)
    
    query = """
    SELECT 
        product_id,
        name,
        price,
        rating_avg,
        review_count
    FROM products 
    WHERE category_id = %s 
      AND is_active = TRUE
      AND stock_quantity > 0
    ORDER BY rating_avg DESC, review_count DESC
    LIMIT 20
    """
    
    cursor.execute(query, (category_id,))
    results = cursor.fetchall()
    
    # Store in cache
    redis_client.setex(cache_key, cache_ttl, json.dumps(results, default=str))
    
    cursor.close()
    connection.close()
    
    return results

# Cache invalidation on product updates
def update_product_and_invalidate_cache(product_id, **updates):
    # Update product
    connection = mysql.connector.connect(**db_config)
    cursor = connection.cursor()
    
    # Get category_id for cache invalidation
    cursor.execute("SELECT category_id FROM products WHERE product_id = %s", (product_id,))
    category_id = cursor.fetchone()[0]
    
    # Update product (example)
    if 'price' in updates:
        cursor.execute("UPDATE products SET price = %s WHERE product_id = %s", 
                      (updates['price'], product_id))
    
    connection.commit()
    cursor.close()
    connection.close()
    
    # Invalidate related caches
    cache_keys_to_delete = [
        f"popular_products_{category_id}",
        f"product_details_{product_id}",
        "homepage_featured_products"
    ]
    
    redis_client.delete(*cache_keys_to_delete)
```

### **2. Database Query Cache:**

#### **MySQL Query Cache Optimization:**
```sql
-- Enable and configure query cache
SET GLOBAL query_cache_type = ON;
SET GLOBAL query_cache_size = 268435456;  -- 256MB

-- Optimize queries for query cache
-- Use consistent formatting for better cache hits
SELECT /* CACHE */ 
    product_id, 
    name, 
    price, 
    rating_avg
FROM products 
WHERE category_id = 1 
  AND is_active = TRUE
ORDER BY rating_avg DESC
LIMIT 10;

-- Monitor query cache performance
SHOW STATUS LIKE 'Qcache%';

-- Cache hit ratio should be > 80%
SELECT 
    ROUND(
        (SELECT VARIABLE_VALUE FROM performance_schema.global_status WHERE VARIABLE_NAME = 'Qcache_hits') /
        (SELECT VARIABLE_VALUE FROM performance_schema.global_status WHERE VARIABLE_NAME = 'Qcache_hits' + 
         SELECT VARIABLE_VALUE FROM performance_schema.global_status WHERE VARIABLE_NAME = 'Qcache_inserts' +
         SELECT VARIABLE_VALUE FROM performance_schema.global_status WHERE VARIABLE_NAME = 'Qcache_not_cached') * 100, 2
    ) as query_cache_hit_ratio_percent;
```

---

## ðŸ“‹ **Performance Testing & Load Testing**

### **1. Database Load Testing:**

#### **Simulating High Traffic:**
```python
import concurrent.futures
import time
import random
import mysql.connector.pooling

# Connection pool for load testing
pool_config = {
    'pool_name': 'load_test_pool',
    'pool_size': 50,
    'host': 'localhost',
    'database': 'ecommerce',
    'user': 'test_user',
    'password': 'test_password'
}

pool = mysql.connector.pooling.MySQLConnectionPool(**pool_config)

def simulate_product_search():
    """Simulate a product search query"""
    connection = pool.get_connection()
    cursor = connection.cursor()
    
    try:
        category_id = random.randint(1, 10)
        price_min = random.randint(1000, 10000)
        price_max = price_min + random.randint(5000, 20000)
        
        start_time = time.time()
        
        cursor.execute("""
            SELECT product_id, name, price, rating_avg
            FROM products 
            WHERE category_id = %s 
              AND price BETWEEN %s AND %s
              AND is_active = TRUE
            ORDER BY rating_avg DESC
            LIMIT 20
        """, (category_id, price_min, price_max))
        
        results = cursor.fetchall()
        end_time = time.time()
        
        return {
            'query_time': end_time - start_time,
            'results_count': len(results),
            'success': True
        }
        
    except Exception as e:
        return {
            'query_time': 0,
            'results_count': 0,
            'success': False,
            'error': str(e)
        }
    finally:
        cursor.close()
        connection.close()

def run_load_test(concurrent_users=100, duration_seconds=60):
    """Run load test with specified parameters"""
    start_test_time = time.time()
    results = []
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_users) as executor:
        while time.time() - start_test_time < duration_seconds:
            # Submit batch of queries
            futures = [executor.submit(simulate_product_search) for _ in range(concurrent_users)]
            
            # Collect results
            for future in concurrent.futures.as_completed(futures):
                result = future.result()
                results.append(result)
            
            time.sleep(0.1)  # Small delay between batches
    
    # Analyze results
    successful_queries = [r for r in results if r['success']]
    failed_queries = [r for r in results if not r['success']]
    
    if successful_queries:
        avg_query_time = sum(r['query_time'] for r in successful_queries) / len(successful_queries)
        max_query_time = max(r['query_time'] for r in successful_queries)
        min_query_time = min(r['query_time'] for r in successful_queries)
    else:
        avg_query_time = max_query_time = min_query_time = 0
    
    print(f"Load Test Results:")
    print(f"Total Queries: {len(results)}")
    print(f"Successful: {len(successful_queries)}")
    print(f"Failed: {len(failed_queries)}")
    print(f"Success Rate: {len(successful_queries)/len(results)*100:.2f}%")
    print(f"Average Query Time: {avg_query_time:.3f}s")
    print(f"Min Query Time: {min_query_time:.3f}s")
    print(f"Max Query Time: {max_query_time:.3f}s")
    print(f"Queries per Second: {len(results)/duration_seconds:.2f}")

# Run the load test
if __name__ == "__main__":
    run_load_test(concurrent_users=50, duration_seconds=30)
```

---

## ðŸ† **Production-Ready Performance Checklist**

### **ðŸ“‹ Database Performance Checklist:**

#### **âœ… Indexing:**
- [ ] All WHERE clause columns are indexed
- [ ] Composite indexes follow optimal column order
- [ ] No unused indexes consuming space
- [ ] Full-text indexes for search functionality
- [ ] Foreign key indexes are present

#### **âœ… Query Optimization:**
- [ ] All queries execute in < 100ms
- [ ] No queries scan entire tables
- [ ] JOINs use proper indexes
- [ ] LIMIT is used for pagination
- [ ] Subqueries are optimized or replaced with JOINs

#### **âœ… Configuration:**
- [ ] InnoDB buffer pool size optimized
- [ ] Connection limits set appropriately
- [ ] Query cache enabled and tuned
- [ ] Slow query log enabled
- [ ] Binary logging configured for replication

#### **âœ… Monitoring:**
- [ ] Performance monitoring tools in place
- [ ] Automated alerts for slow queries
- [ ] Resource usage monitoring
- [ ] Connection count tracking
- [ ] Cache hit ratio monitoring

#### **âœ… Caching:**
- [ ] Query result caching implemented
- [ ] Redis/Memcached for session data
- [ ] Database connection pooling
- [ ] Static content caching
- [ ] Cache invalidation strategy

---

## ðŸŽ¯ **Next Steps**

**Completed:** âœ… Performance optimization mastered with production-level techniques  
**Next Module:** **06-Real-Projects** - Build complete applications with all learned concepts

**Real Learning:** Ab tumhe pata hai ki **high-traffic applications** kaise **millions of users** handle karte hain! **Database optimization, caching, monitoring** - sab **production-ready skills** mil gaye! ðŸ’ª

**Ready for Real Projects?** Complete **end-to-end applications** banayenge jo **industry standards** follow karte hain! ðŸš€ðŸ’¼
